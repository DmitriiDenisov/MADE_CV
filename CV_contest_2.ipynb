{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CV_contest_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9248806aecfc472eb511921079952e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cc797b46489f4fe5b27465506f960fa7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a4c27a555c324de7b3a0b5ad7206bba6",
              "IPY_MODEL_e8bb08a9a7ba4cf6bfac9d765ae1599e"
            ]
          }
        },
        "cc797b46489f4fe5b27465506f960fa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4c27a555c324de7b3a0b5ad7206bba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_793e2a113a6744ef8706152716736137",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 178090079,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 178090079,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_154a61baed344781a1b4c243299a9bd0"
          }
        },
        "e8bb08a9a7ba4cf6bfac9d765ae1599e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_63dc6892f516480ebf6bcf01ada9ac20",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170M/170M [00:24&lt;00:00, 7.31MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f8e45f64b305410fb40a2777f6c9f3fc"
          }
        },
        "793e2a113a6744ef8706152716736137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "154a61baed344781a1b4c243299a9bd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "63dc6892f516480ebf6bcf01ada9ac20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f8e45f64b305410fb40a2777f6c9f3fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f107f34d057049e4a7b446aa98b03f6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_272f2ff8a89e4628aa3dabb53af9c1f0",
              "IPY_MODEL_d3f29bc1d76e467180789c1a4635d4d5"
            ],
            "layout": "IPY_MODEL_fb9f2991e1f34b79a3fe558f50ee0c81"
          }
        },
        "fb9f2991e1f34b79a3fe558f50ee0c81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "272f2ff8a89e4628aa3dabb53af9c1f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3060d3bf7954da5a107a2d222be6253",
            "max": 3188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_873b174c370e46fbbc200d32815646fe",
            "value": 3188
          }
        },
        "d3f29bc1d76e467180789c1a4635d4d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9387f668d3e483fa7613c058222b34d",
            "placeholder": "​",
            "style": "IPY_MODEL_f821bb8778e845eabf14b465af6c7985",
            "value": " 3188/3188 [02:47&lt;00:00, 18.98it/s]"
          }
        },
        "873b174c370e46fbbc200d32815646fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "a3060d3bf7954da5a107a2d222be6253": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f821bb8778e845eabf14b465af6c7985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9387f668d3e483fa7613c058222b34d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitriiDenisov/MADE_CV/blob/master/CV_contest_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "87TOtL0O6C3T"
      },
      "source": [
        "# Распознавания автомобильных номеров. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NCKiSNyI6C3X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "116b55d7-4ea1-4084-a627-217124d9a9e2"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import json\n",
        "import glob\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.path import Path\n",
        "import seaborn as sns\n",
        "import tqdm\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision import transforms\n",
        "\n",
        "import PIL\n",
        "from PIL import Image, ImageDraw\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K5fJ7e5f6C3g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff52df39-37ae-463f-ba24-eeb1eb947bc8"
      },
      "source": [
        "torch.__version__, torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1.5.0+cu101', True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DBJHmNW66C3p",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NWFEHe6z6C3w"
      },
      "source": [
        "### Сейчас будет много вспомогательных функций, которые можно промотать\n",
        "\n",
        "Чтобы было наглядно и не приходилось лезть в модули, чтобы посмотреть, какая функция что делает, оставил для наглядности пока что все в ноутбуке. Можно аккуратно перенести в модули :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RiZUZedC6C3x",
        "colab": {}
      },
      "source": [
        "# Бинарный поиск для приближения предсказанной маски 4-хугольником\n",
        "def simplify_contour(contour, n_corners=4):\n",
        "    n_iter, max_iter = 0, 1000\n",
        "    lb, ub = 0., 1.\n",
        "\n",
        "    while True:\n",
        "        n_iter += 1\n",
        "        if n_iter > max_iter:\n",
        "            print('simplify_contour didnt converege')\n",
        "            return None\n",
        "\n",
        "        k = (lb + ub)/2.\n",
        "        eps = k*cv2.arcLength(contour, True)\n",
        "        approx = cv2.approxPolyDP(contour, eps, True)\n",
        "\n",
        "        if len(approx) > n_corners:\n",
        "            lb = (lb + ub)/2.\n",
        "        elif len(approx) < n_corners:\n",
        "            ub = (lb + ub)/2.\n",
        "        else:\n",
        "            return approx\n",
        "\n",
        "# Отображаем 4-хугольник в прямоугольник \n",
        "# Спасибо ulebok за идею \n",
        "# И вот этим ребятам за реализацию: https://www.pyimagesearch.com/2014/08/25/4-point-opencv-getperspective-transform-example/\n",
        "def four_point_transform(image, pts):\n",
        "    \n",
        "    rect = order_points(pts)\n",
        "    \n",
        "    tl, tr, br, bl = pts\n",
        "    \n",
        "    width_1 = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
        "    width_2 = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
        "    max_width = max(int(width_1), int(width_2))\n",
        "    \n",
        "    height_1 = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
        "    height_2 = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
        "    max_height = max(int(height_1), int(height_2))\n",
        "    \n",
        "    dst = np.array([\n",
        "        [0, 0],\n",
        "        [max_width, 0],\n",
        "        [max_width, max_height],\n",
        "        [0, max_height]], dtype = \"float32\")\n",
        "    \n",
        "    M = cv2.getPerspectiveTransform(rect, dst)\n",
        "    warped = cv2.warpPerspective(image, M, (max_width, max_height))\n",
        "    return warped\n",
        "\n",
        "def order_points(pts):\n",
        "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
        "    \n",
        "    s = pts.sum(axis = 1)\n",
        "    rect[0] = pts[np.argmin(s)]\n",
        "    rect[2] = pts[np.argmax(s)]\n",
        "    \n",
        "    diff = np.diff(pts, axis = 1)\n",
        "    rect[1] = pts[np.argmin(diff)]\n",
        "    rect[3] = pts[np.argmax(diff)]\n",
        "    \n",
        "    return rect\n",
        "\n",
        "\n",
        "# Визуализируем детекцию (4 точки, bounding box и приближенный по маске контур)\n",
        "def visualize_prediction_plate(file, model, device='cuda', verbose=True, thresh=0.0, \n",
        "                               n_colors=None, id_to_name=None, draw=True):\n",
        "    img = Image.open(file)\n",
        "    img_tensor = my_transforms(img)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model([img_tensor.to(device)])\n",
        "    prediction = predictions[0]\n",
        "    \n",
        "    if n_colors is None:\n",
        "        n_colors = model.roi_heads.box_predictor.cls_score.out_features\n",
        "    \n",
        "    palette = sns.color_palette(None, n_colors)\n",
        "    \n",
        "    img = cv2.imread(file, cv2.COLOR_BGR2RGB)\n",
        "    h, w = img.shape[:2]\n",
        "    image = img\n",
        "    \n",
        "    blackImg = np.zeros(image.shape, image.dtype)\n",
        "    blackImg[:,:] = (0, 0, 0)\n",
        "    for i in range(len(prediction['boxes'])):\n",
        "        x_min, y_min, x_max, y_max = map(int, prediction['boxes'][i].tolist())\n",
        "        label = int(prediction['labels'][i].cpu())\n",
        "        score = float(prediction['scores'][i].cpu())\n",
        "        mask = prediction['masks'][i][0, :, :].cpu().numpy()\n",
        "        name = id_to_name[label]\n",
        "        color = palette[label]\n",
        "        \n",
        "        if verbose and draw:\n",
        "            if score > thresh:\n",
        "                print ('Class: {}, Confidence: {}'.format(name, score))\n",
        "        if score > thresh and draw:            \n",
        "            crop_img = image[y_min:y_max, x_min:x_max]\n",
        "            print('Bounding box:')\n",
        "            show_image(crop_img, figsize=(10, 2))\n",
        "            \n",
        "            contours, _ = cv2.findContours((mask > 0.05).astype(np.uint8), 1, 1)\n",
        "            approx = simplify_contour(contours[0], n_corners=4)\n",
        "            \n",
        "            if approx is None:\n",
        "                x0, y0 = x_min, y_min\n",
        "                x1, y1 = x_max, y_min\n",
        "                x2, y2 = x_min, y_max\n",
        "                x3, y3 = x_max, y_max\n",
        "#                 points = [[x_min, y_min], [x_min, y_max], [x_max, y_min],[x_max, y_max]]\n",
        "            else:\n",
        "                x0, y0 = approx[0][0][0], approx[0][0][1]\n",
        "                x1, y1 = approx[1][0][0], approx[1][0][1]\n",
        "                x2, y2 = approx[2][0][0], approx[2][0][1]\n",
        "                x3, y3 = approx[3][0][0], approx[3][0][1]\n",
        "                \n",
        "            points = [[x0, y0], [x2, y2], [x1, y1],[x3, y3]]\n",
        "            \n",
        "            \n",
        "            points = np.array(points)\n",
        "            crop_mask_img = four_point_transform(img, points)\n",
        "            print('Rotated img:')\n",
        "            crop_mask_img = cv2.resize(crop_mask_img, (320, 64), interpolation=cv2.INTER_AREA)\n",
        "            show_image(crop_mask_img, figsize=(10, 2))\n",
        "            if approx is not None:\n",
        "                cv2.drawContours(image, [approx], 0, (255,0,255), 3)\n",
        "            image = cv2.circle(image, (x0, y0), radius=5, color=(0, 0, 255), thickness=-1)\n",
        "            image = cv2.circle(image, (x1, y1), radius=5, color=(0, 0, 255), thickness=-1)\n",
        "            image = cv2.circle(image, (x2, y2), radius=5, color=(0, 0, 255), thickness=-1)\n",
        "            image = cv2.circle(image, (x3, y3), radius=5, color=(0, 0, 255), thickness=-1)\n",
        "            \n",
        "            image = cv2.rectangle(image, (x_min, y_min), (x_max, y_max), np.array(color) * 255, 2)\n",
        "    if draw:         \n",
        "      show_image(image)\n",
        "    return prediction\n",
        "\n",
        "# Просто показать картинку. С семинара\n",
        "def show_image(image, figsize=(16, 9), reverse=True):\n",
        "    plt.figure(figsize=figsize)\n",
        "    if reverse:\n",
        "        plt.imshow(image[...,::-1])\n",
        "    else:\n",
        "        plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "# Переводит предсказания модели в текст. С семинара\n",
        "def decode(pred, alphabet):\n",
        "    pred = pred.permute(1, 0, 2).cpu().data.numpy()\n",
        "    outputs = []\n",
        "    for i in range(len(pred)):\n",
        "        outputs.append(pred_to_string(pred[i], alphabet))\n",
        "    return outputs\n",
        "\n",
        "def pred_to_string(pred, alphabet):\n",
        "    seq = []\n",
        "    for i in range(len(pred)):\n",
        "        label = np.argmax(pred[i])\n",
        "        seq.append(label - 1)\n",
        "    out = []\n",
        "    for i in range(len(seq)):\n",
        "        if len(out) == 0:\n",
        "            if seq[i] != -1:\n",
        "                out.append(seq[i])\n",
        "        else:\n",
        "            if seq[i] != -1 and seq[i] != seq[i - 1]:\n",
        "                out.append(seq[i])\n",
        "    out = ''.join([alphabet[c] for c in out])\n",
        "    return out\n",
        "    \n",
        "\n",
        "        \n",
        "def load_json(file):\n",
        "    with open(file, 'r') as f:\n",
        "        return json.load(f)\n",
        "    \n",
        "# Чтобы без проблем сериализовывать json. Без него есть нюансы\n",
        "class npEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.int32):\n",
        "            return int(obj)\n",
        "        return json.JSONEncoder.default(self, obj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kdnRxf506C37",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-khQOOdOA3y3",
        "colab": {}
      },
      "source": [
        "abc = \"0123456789ABEKMHOPCTYXСВХАОВТКЕМОРН\"\n",
        "from string import digits\n",
        "\n",
        "def compute_mask(text):\n",
        "    \"\"\"Compute letter-digit mask of text.\n",
        "    Accepts string of text. \n",
        "    Returns string of the same length but with every letter replaced by 'L' and every digit replaced by 'D'.\n",
        "    e.g. 'E506EC152' -> 'LDDDLLDDD'.\n",
        "    Returns None if non-letter and non-digit character met in text.\n",
        "    \"\"\"\n",
        "    mask = []\n",
        "    for char in text.upper():\n",
        "        if char in digits:\n",
        "            mask.append(\"D\")\n",
        "        elif char in abc:\n",
        "            mask.append(\"L\")\n",
        "        else:\n",
        "            return None\n",
        "    return \"\".join(mask)\n",
        "\n",
        "def check_in_alphabet(text, alphabet=abc):\n",
        "    \"\"\"Check if all chars in text come from alphabet.\n",
        "    Accepts string of text and string of alphabet. \n",
        "    Returns True if all chars in text are from alphabet and False else.\n",
        "    \"\"\"\n",
        "    for char in text.upper():\n",
        "        if char not in alphabet:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def filter_data(config):\n",
        "    \"\"\"Filter config keeping only items with correct text.\n",
        "    Accepts list of items.\n",
        "    Returns new list.\n",
        "    \"\"\"\n",
        "    config_filtered = []\n",
        "    for item in config:\n",
        "        text = item['nums'][0][\"text\"].upper()\n",
        "        mask = compute_mask(text)\n",
        "        if check_in_alphabet(text) and (mask == \"LDDDLLDD\" or mask == \"LDDDLLDDD\"):\n",
        "            config_filtered.append({\"file\": item[\"file\"],\n",
        "                                   \"nums\": item['nums']\n",
        "                                    })\n",
        "    return config_filtered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d5kfTjEL6C4H",
        "colab": {}
      },
      "source": [
        "DATA_PATH = './data/'\n",
        "TRAIN_SIZE = 0.9\n",
        "BATCH_SIZE = 8\n",
        "BATCH_SIZE_OCR = 8\n",
        "DETECTOR_MODEL_PATH = 'detect_model.pt'\n",
        "OCR_MODEL_PATH = 'ocr_model.pt'\n",
        "\n",
        "all_marks = load_json(os.path.join(DATA_PATH, 'train.json'))\n",
        "all_marks = filter_data(all_marks)\n",
        "test_start = int(TRAIN_SIZE * len(all_marks))\n",
        "train_marks = all_marks[:test_start]\n",
        "val_marks = all_marks[test_start:]\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wK-PazjLAoSP",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IxIA4vjJ6C4T"
      },
      "source": [
        "# 1. Находим номера"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vKAAPKiH6C4W"
      },
      "source": [
        "### a) Модель для детекции\n",
        "\n",
        "В задании есть данные о 4 точках, которые задают номер. Эти 4 точки - почти всегда не прямоугольник, а произвольный четырехугольник. Будем предсказывать:\n",
        "\n",
        "- bounding box, который окружает точки (детекция)\n",
        "- маску, заполненную тем, что внутри 4-х точек (сегментация)\n",
        "\n",
        "Поэтому, возьмем maskrcnn. Будем обучать несколько последних солев. Этого с запасом хватает."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a0-qtbxk6C4X",
        "colab": {}
      },
      "source": [
        "def get_detector_model():\n",
        "    \n",
        "    model = models.detection.maskrcnn_resnet50_fpn(\n",
        "        pretrained=True, \n",
        "        pretrained_backbone=True,\n",
        "        progress=True, \n",
        "        num_classes=91, \n",
        "    )\n",
        "\n",
        "    num_classes = 2\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    \n",
        "    box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    model.roi_heads.box_predictor = box_predictor\n",
        "    \n",
        "    mask_predictor = MaskRCNNPredictor(256, 256, num_classes)\n",
        "    model.roi_heads.mask_predictor = mask_predictor\n",
        "\n",
        "    # Заморозим все слои кроме последних\n",
        "    \n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "        \n",
        "    for param in model.backbone.fpn.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for param in model.rpn.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for param in model.roi_heads.parameters():\n",
        "        param.requires_grad = True\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fhd-zJhK6C4g"
      },
      "source": [
        "### b) Датасет для детекции"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CAd5Mmlm6C4h",
        "colab": {}
      },
      "source": [
        "class DetectionDataset(Dataset):\n",
        "    def __init__(self, marks, img_folder, transforms=None):\n",
        "        \n",
        "        self.marks = marks\n",
        "        self.img_folder = img_folder\n",
        "        self.transforms = transforms\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.marks[idx]\n",
        "        img_path = f'{self.img_folder}{item[\"file\"]}'\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        w, h = img.size\n",
        "        \n",
        "        box_coords = item['nums']\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        masks = []\n",
        "        for box in box_coords:\n",
        "            points = np.array(box['box'])  \n",
        "            x0, y0 = np.min(points[:, 0]), np.min(points[:, 1])\n",
        "            x2, y2 = np.max(points[:, 0]), np.max(points[:, 1])\n",
        "            boxes.append([x0, y0, x2, y2])\n",
        "            labels.append(1)\n",
        "            \n",
        "            # Здесь мы наши 4 точки превращаем в маску\n",
        "            # Это нужно, чтобы кроме bounding box предсказывать и, соответственно, маску :)\n",
        "            nx, ny = w, h\n",
        "            poly_verts = points\n",
        "            x, y = np.meshgrid(np.arange(nx), np.arange(ny))\n",
        "            x, y = x.flatten(), y.flatten()\n",
        "            points = np.vstack((x,y)).T\n",
        "            path = Path(poly_verts)\n",
        "            grid = path.contains_points(points)\n",
        "            grid = grid.reshape((ny,nx)).astype(int)\n",
        "            masks.append(grid)\n",
        "            \n",
        "        boxes = torch.as_tensor(boxes)\n",
        "        labels = torch.as_tensor(labels)\n",
        "        masks = torch.as_tensor(masks)\n",
        "        \n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'masks': masks,\n",
        "        }\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "        \n",
        "        return img, target\n",
        "    \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.marks)\n",
        "    \n",
        "my_transforms = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = DetectionDataset(\n",
        "    marks=train_marks, \n",
        "    img_folder='data/', \n",
        "    transforms=my_transforms\n",
        ")\n",
        "val_dataset = DetectionDataset(\n",
        "    marks=val_marks, \n",
        "    img_folder='data/', \n",
        "    transforms=my_transforms\n",
        ")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    drop_last=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn, \n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    drop_last=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn, \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y3YB4aN96C4z"
      },
      "source": [
        "### c) Обучаем модель для детекции"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hbxwpBZJ6C41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "9248806aecfc472eb511921079952e29",
            "cc797b46489f4fe5b27465506f960fa7",
            "a4c27a555c324de7b3a0b5ad7206bba6",
            "e8bb08a9a7ba4cf6bfac9d765ae1599e",
            "793e2a113a6744ef8706152716736137",
            "154a61baed344781a1b4c243299a9bd0",
            "63dc6892f516480ebf6bcf01ada9ac20",
            "f8e45f64b305410fb40a2777f6c9f3fc"
          ]
        },
        "outputId": "2d741b7c-87c8-4914-c2ee-979b7720e205"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "model = get_detector_model()\n",
        "# model.load_state_dict(torch.load('detect_model.pth'))\n",
        "model.to(device);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9248806aecfc472eb511921079952e29",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=178090079.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hmsudRXI6C49"
      },
      "source": [
        "Валидироваться на чем-то нет смысла, ибо лосс перестает падать еще до того момента, как пройдет 1-я эпоха. Т.е. лосс на трейне вполне валидный, ибо модель видит данные в первый раз."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KA89ydlg6C4-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "55dd1de6-e85c-4285-aba9-6b12ad44fbde"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=25, \n",
        "                                                       factor=0.5, verbose=True)\n",
        "\n",
        "tr_loss = []\n",
        "val_loss = []\n",
        "cur_best_loss = np.inf\n",
        "NUM_EPOCH = 3\n",
        "# Train\n",
        "for epoch in range(NUM_EPOCH):\n",
        "  model.train()\n",
        "  print(f'Epoch {epoch}')\n",
        "  print_loss = []\n",
        "  for i, (images, targets) in enumerate(tqdm.tqdm(train_loader,\n",
        "                                                  total=len(train_loader),\n",
        "                                                  leave=False, position=0)):\n",
        "      images = [image.to(device) for image in images]\n",
        "      targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "      loss_dict = model(images, targets)\n",
        "      losses = sum(loss_dict.values())\n",
        "\n",
        "      losses.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      print_loss.append(losses.item())\n",
        "      if (i + 1) % 20 == 0:\n",
        "        mean_loss = np.mean(print_loss)\n",
        "        scheduler.step(mean_loss)\n",
        "      if (i + 1) % 500 == 0:\n",
        "        print(f'Loss: {mean_loss:.7f}')\n",
        "  tr_loss.append(mean_loss)\n",
        "\n",
        "# Validation\n",
        "  model.eval()\n",
        "  s = 0\n",
        "  counter = 0\n",
        "  val_loss = []\n",
        "  try:\n",
        "    for i, (images, targets) in enumerate(tqdm.tqdm(val_loader,\n",
        "                                                  total=len(val_loader),\n",
        "                                                  leave=False, position=0)):\n",
        "        images = [image.to(device) for image in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        with torch.no_grad():\n",
        "            loss_dict2 = model(images, targets)\n",
        "        for i in range(len(loss_dict2)):\n",
        "          for val in loss_dict2[i]['scores']: \n",
        "            s += 1 - val\n",
        "            counter += 1\n",
        "        val_loss.append(s/counter)\n",
        "  except:\n",
        "    print('Err')\n",
        "  cur_loss = sum(val_loss) / len(val_loss)\n",
        "  print(f'Epoch {epoch} validation loss = {cur_loss}')\n",
        "  \n",
        "  if cur_loss < cur_best_loss:\n",
        "    cur_best_loss = cur_loss\n",
        "    with open(f\"detect_model.pth\", \"wb\") as fp:\n",
        "      torch.save(model.state_dict(), fp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2870 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 500/2870 [10:06<45:58,  1.16s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.1612457\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 35%|███▍      | 1000/2870 [20:18<36:07,  1.16s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.1573175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 1500/2870 [30:27<26:04,  1.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.1563171\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 56%|█████▋    | 1621/2870 [32:54<23:58,  1.15s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VIAbw_U1Z_vc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2941965f-c74d-4ee1-d6e3-ce43418d29e1"
      },
      "source": [
        "cur_best_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inf"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F1dT1_t4lZE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ae7143f-937d-4a9f-ae50-4dad5d07142f"
      },
      "source": [
        "1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KpnMlpG7jF3b",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load('detect_model.pth'))\n",
        "model.to(device);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D8GhjYdF6C5L",
        "colab": {}
      },
      "source": [
        "test_images = glob.glob(os.path.join(DATA_PATH, 'test/*'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bQ8x-8ZJuAcI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "14a0b99c-2ae9-41c6-dede-f2750d9dcd0f"
      },
      "source": [
        "len(test_images)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3188"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qCTWXUOd6C5R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c7afda33-f07a-4b44-d56e-76cdbf3adba2"
      },
      "source": [
        "ids = set()\n",
        "ids2 = set()\n",
        "ids3 = set()\n",
        "ids4 = set()\n",
        "ids5 = set()\n",
        "for i in tqdm.tqdm(range(len(test_images))):\n",
        "  try:\n",
        "    k = visualize_prediction_plate(test_images[i], model, \n",
        "                           id_to_name={1: 'plate'}, thresh=0.6, draw=False)['scores']\n",
        "  except:\n",
        "    k=[]\n",
        "  for j in k:\n",
        "    if j >0.7 and j <0.75:\n",
        "      ids.add(i)\n",
        "    if j >=0.75 and j <0.8:\n",
        "      ids2.add(i)\n",
        "    if j >=0.8 and j <0.85:\n",
        "      ids3.add(i)\n",
        "    if j >=0.85 and j <0.9:\n",
        "      ids4.add(i)\n",
        "    if j >=0.9 and j <0.95:\n",
        "      ids5.add(i)\n",
        "ids = list(ids)\n",
        "ids2 = list(ids2)\n",
        "ids3 = list(ids3)\n",
        "ids4 = list(ids4)\n",
        "ids5 = list(ids5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3188/3188 [07:29<00:00,  7.09it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zKcihISX6C5W",
        "colab": {}
      },
      "source": [
        "for j, i in enumerate(list(ids4)[20:]):\n",
        "  print(j)\n",
        "  visualize_prediction_plate(test_images[i], model, \n",
        "                           id_to_name={1: 'plate'}, thresh=0.85)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2VEAo2cp6C5b"
      },
      "source": [
        "### d) Предсказываем bounding box-ы и маску. \n",
        "\n",
        "- Маску превращаем в 4-угольный полигон. Сохраняем предсказания в json\n",
        "- Если маска не приближается 4-угольником (редко такое бывает, бинарный поиск по гиперпараметру не работает), то просто записываем координаты bounding box "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EVKbxxjo6C5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7fc57d6e-c1bf-4b27-de49-18ff29175bef"
      },
      "source": [
        "THRESHOLD_SCORE = 0.89\n",
        "TRESHOLD_MASK = 0.05\n",
        "\n",
        "preds = []\n",
        "model.eval()\n",
        "\n",
        "\n",
        "for file in tqdm.tqdm(test_images, total=len(test_images),\n",
        "                                                  leave=False, position=0):\n",
        "    img = Image.open(file).convert('RGB')\n",
        "    img_tensor = my_transforms(img)\n",
        "    with torch.no_grad():\n",
        "        predictions = model([img_tensor.to(device)])\n",
        "    prediction = predictions[0]\n",
        "\n",
        "    pred = dict()\n",
        "    pred['file'] = file\n",
        "    pred['nums'] = []\n",
        "\n",
        "    for i in range(len(prediction['boxes'])):\n",
        "        x_min, y_min, x_max, y_max = map(int, prediction['boxes'][i].tolist())\n",
        "        label = int(prediction['labels'][i].cpu())\n",
        "        score = float(prediction['scores'][i].cpu())\n",
        "        mask = prediction['masks'][i][0, :, :].cpu().numpy()\n",
        "\n",
        "        if score > THRESHOLD_SCORE:            \n",
        "            contours,_ = cv2.findContours((mask > TRESHOLD_MASK).astype(np.uint8), 1, 1)\n",
        "            approx = simplify_contour(contours[0], n_corners=4)\n",
        "            \n",
        "            if approx is None:\n",
        "                x0, y0 = x_min, y_min\n",
        "                x1, y1 = x_max, y_min\n",
        "                x2, y2 = x_min, y_max\n",
        "                x3, y3 = x_max, y_max\n",
        "            else:\n",
        "                x0, y0 = approx[0][0][0], approx[0][0][1]\n",
        "                x1, y1 = approx[1][0][0], approx[1][0][1]\n",
        "                x2, y2 = approx[2][0][0], approx[2][0][1]\n",
        "                x3, y3 = approx[3][0][0], approx[3][0][1]\n",
        "                \n",
        "            points = [[x0, y0], [x2, y2], [x1, y1],[x3, y3]]\n",
        "\n",
        "            pred['nums'].append({\n",
        "                'box': points,\n",
        "                'bbox': [x_min, y_min, x_max, y_max],\n",
        "            })\n",
        "\n",
        "    preds.append(pred)   \n",
        "\n",
        "    \n",
        "with open(os.path.join(DATA_PATH, 'test.json'), 'w') as json_file:\n",
        "    json.dump(preds, json_file, cls=npEncoder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OLJM1pjw6C5m"
      },
      "source": [
        "# 2. Распознаем номера"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ajpFOI3J6C5p"
      },
      "source": [
        "### a) Датасет для распознавания номеров\n",
        "\n",
        "Из особенностей - на каждый номер мы генерируем bounding box + вырезаем по точкам и превращаем в прямоугольник наш 4-угольник по данным точкам. Т.е. 2 картинки на номер."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mlictwXp6C5p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "b0d0aae5-c44f-4a8e-8ae4-82d6f52baa03"
      },
      "source": [
        "class OCRDataset(Dataset):\n",
        "    def __init__(self, marks, img_folder, alphabet, transforms=None):\n",
        "        ocr_marks = []\n",
        "        for items in marks:\n",
        "            file_path = items['file']\n",
        "            for box in items['nums']:\n",
        "                \n",
        "                ocr_marks.append({\n",
        "                    'file': file_path,\n",
        "                    'box': np.clip(box['box'], 0, None).tolist(),\n",
        "                    'text': box['text'],\n",
        "                    'boxed': False,\n",
        "                })\n",
        "                            \n",
        "                # Добавим точки, запакованные в BoundingBox. \n",
        "                # Вместо аугментации rotate. Датасет будет в 2 раза больше\n",
        "                \n",
        "                #Клипаем, ибо есть отрицательные координаты\n",
        "                points = np.clip(box['box'], 0, None) \n",
        "                x0, y0 = np.min(points[:, 0]), np.min(points[:, 1])\n",
        "                x2, y2 = np.max(points[:, 0]), np.max(points[:, 1])\n",
        "\n",
        "                ocr_marks.append({\n",
        "                    'file': file_path,\n",
        "                    'box': [x0, y0, x2, y2],\n",
        "                    'text': box['text'],\n",
        "                    'boxed': True,\n",
        "                })\n",
        "                \n",
        "        self.marks = ocr_marks\n",
        "        self.img_folder = img_folder\n",
        "        self.transforms = transforms\n",
        "        self.alphabet = alphabet\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.marks[idx]\n",
        "        img_path = os.path.join(self.img_folder, item[\"file\"])\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        if item['boxed']:\n",
        "            x_min, y_min, x_max, y_max = item['box']\n",
        "            img = img[y_min:y_max, x_min:x_max]\n",
        "        else:\n",
        "            points = np.clip(np.array(item['box']), 0, None)\n",
        "            img = four_point_transform(img, points)\n",
        "            \n",
        "        text = item['text']\n",
        "        seq = [self.alphabet.find(char) + 1 for char in text]\n",
        "        seq_len = len(seq)\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        output = {\n",
        "            'img': img,\n",
        "            'text': text,\n",
        "            'seq': seq,\n",
        "            'seq_len': seq_len\n",
        "        }\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.marks)\n",
        "    \n",
        "    \n",
        "class Resize(object):\n",
        "    def __init__(self, size=(320, 64)):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, img):\n",
        "\n",
        "        w_from, h_from = img.shape[1], img.shape[0]\n",
        "        w_to, h_to = self.size\n",
        "        \n",
        "        # Сделаем разную интерполяцию при увеличении и уменьшении\n",
        "        # Если увеличиваем картинку, меняем интерполяцию\n",
        "        interpolation = cv2.INTER_AREA\n",
        "        if w_to > w_from:\n",
        "            interpolation = cv2.INTER_CUBIC\n",
        "        \n",
        "        img = cv2.resize(img, dsize=self.size, interpolation=interpolation)\n",
        "        return img\n",
        "    \n",
        "my_ocr_transforms = transforms.Compose([\n",
        "    Resize(size=(320, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def get_vocab_from_marks(marks):\n",
        "    train_texts = []\n",
        "    for item in marks:\n",
        "        for num in item['nums']:\n",
        "            train_texts.append(num['text'])\n",
        "\n",
        "    counts = Counter(''.join(train_texts))\n",
        "    alphabet = ''.join(set(''.join(train_texts)))\n",
        "    corted_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    char_to_idx = {item[0]: idx + 1 for idx, item in enumerate(corted_counts)}\n",
        "    idx_to_char = {idx:char for char, idx in char_to_idx.items()}\n",
        "    return char_to_idx, idx_to_char, alphabet\n",
        "\n",
        "char_to_idx, idx_to_char, alphabet = get_vocab_from_marks(train_marks)\n",
        "\n",
        "train_ocr_dataset = OCRDataset(\n",
        "    marks=train_marks, \n",
        "    img_folder=DATA_PATH, \n",
        "    alphabet=alphabet,\n",
        "    transforms=my_ocr_transforms\n",
        ")\n",
        "val_ocr_dataset = OCRDataset(\n",
        "    marks=val_marks, \n",
        "    img_folder=DATA_PATH, \n",
        "    alphabet=alphabet,\n",
        "    transforms=my_ocr_transforms\n",
        ")\n",
        "\n",
        "def collate_fn_ocr(batch):\n",
        "    \"\"\"Function for torch.utils.data.Dataloader for batch collecting.\n",
        "    Accepts list of dataset __get_item__ return values (dicts).\n",
        "    Returns dict with same keys but values are either torch.Tensors of batched images, sequences, and so.\n",
        "    \"\"\"\n",
        "    images, seqs, seq_lens, texts = [], [], [], []\n",
        "    for sample in batch:\n",
        "        images.append(sample[\"img\"])\n",
        "        seqs.extend(sample[\"seq\"])\n",
        "        seq_lens.append(sample[\"seq_len\"])\n",
        "        texts.append(sample[\"text\"])\n",
        "    images = torch.stack(images)\n",
        "    seqs = torch.Tensor(seqs).int()\n",
        "    seq_lens = torch.Tensor(seq_lens).int()\n",
        "    batch = {\"image\": images, \"seq\": seqs, \"seq_len\": seq_lens, \"text\": texts}\n",
        "    return batch\n",
        "\n",
        "train_ocr_loader = DataLoader(\n",
        "    train_ocr_dataset, \n",
        "    batch_size=BATCH_SIZE_OCR, \n",
        "    drop_last=True,\n",
        "    num_workers=0, # Почему-то у меня виснет DataLoader, если запустить несколько потоков\n",
        "    collate_fn=collate_fn_ocr,\n",
        "    timeout=0,\n",
        "    shuffle=True # Чтобы повернутые дубли картинок не шли подряд\n",
        ")\n",
        "\n",
        "val_ocr_loader = DataLoader(\n",
        "    val_ocr_dataset, \n",
        "    batch_size=BATCH_SIZE_OCR, \n",
        "    drop_last=False,\n",
        "    num_workers=0,\n",
        "    collate_fn=collate_fn_ocr, \n",
        "    timeout=0,\n",
        ")\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ztmuEPI16C5w"
      },
      "source": [
        "### b) Модель для распознования текста номера\n",
        "\n",
        "Взял RCNN из семинара"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TkZBOPXP6C5x",
        "colab": {}
      },
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size=(64, 320), output_len=20):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        \n",
        "        h, w = input_size\n",
        "        resnet = getattr(models, 'resnet18')(pretrained=True)\n",
        "        self.cnn = nn.Sequential(*list(resnet.children())[:-2])\n",
        "        \n",
        "        self.pool = nn.AvgPool2d(kernel_size=(h // 32, 1))        \n",
        "        self.proj = nn.Conv2d(w // 32, output_len, kernel_size=1)\n",
        "  \n",
        "        self.num_output_features = self.cnn[-1][-1].bn2.num_features    \n",
        "    \n",
        "    def apply_projection(self, x):\n",
        "        \"\"\"Use convolution to increase width of a features.\n",
        "        Accepts tensor of features (shaped B x C x H x W).\n",
        "        Returns new tensor of features (shaped B x C x H x W').\n",
        "        \"\"\"\n",
        "        x = x.permute(0, 3, 2, 1).contiguous()\n",
        "        x = self.proj(x)\n",
        "        x = x.permute(0, 2, 3, 1).contiguous()\n",
        "        return x\n",
        "   \n",
        "    def forward(self, x):\n",
        "        # Apply conv layers\n",
        "        features = self.cnn(x)\n",
        "        \n",
        "        # Pool to make height == 1\n",
        "        features = self.pool(features)\n",
        "        \n",
        "        # Apply projection to increase width\n",
        "        features = self.apply_projection(features)\n",
        "        \n",
        "        return features\n",
        "    \n",
        "class SequencePredictor(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3, bidirectional=False):\n",
        "        super(SequencePredictor, self).__init__()\n",
        "        \n",
        "        self.num_classes = num_classes        \n",
        "        self.rnn = nn.GRU(input_size=input_size,\n",
        "                       hidden_size=hidden_size,\n",
        "                       num_layers=num_layers,\n",
        "                       dropout=dropout,\n",
        "                       bidirectional=bidirectional)\n",
        "        \n",
        "        fc_in = hidden_size if not bidirectional else 2 * hidden_size\n",
        "        self.fc = nn.Linear(in_features=fc_in,\n",
        "                         out_features=num_classes)\n",
        "    \n",
        "    def _init_hidden_(self, batch_size):\n",
        "        \"\"\"Initialize new tensor of zeroes for RNN hidden state.\n",
        "        Accepts batch size.\n",
        "        Returns tensor of zeros shaped (num_layers * num_directions, batch, hidden_size).\n",
        "        \"\"\"\n",
        "        num_directions = 2 if self.rnn.bidirectional else 1\n",
        "        return torch.zeros(self.rnn.num_layers * num_directions, batch_size, self.rnn.hidden_size)\n",
        "        \n",
        "    def _prepare_features_(self, x):\n",
        "        \"\"\"Change dimensions of x to fit RNN expected input.\n",
        "        Accepts tensor x shaped (B x (C=1) x H x W).\n",
        "        Returns new tensor shaped (W x B x H).\n",
        "        \"\"\"\n",
        "        x = x.squeeze(1)\n",
        "        x = x.permute(2, 0, 1)\n",
        "        return x\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self._prepare_features_(x)\n",
        "        \n",
        "        batch_size = x.size(1)\n",
        "        h_0 = self._init_hidden_(batch_size)\n",
        "        h_0 = h_0.to(x.device)\n",
        "        x, h = self.rnn(x, h_0)\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "    \n",
        "class CRNN(nn.Module):\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        alphabet=alphabet,\n",
        "        cnn_input_size=(64, 320), \n",
        "        cnn_output_len=20,\n",
        "        rnn_hidden_size=128, \n",
        "        rnn_num_layers=2, \n",
        "        rnn_dropout=0.3, \n",
        "        rnn_bidirectional=False\n",
        "    ):\n",
        "        super(CRNN, self).__init__()\n",
        "        self.alphabet = alphabet\n",
        "        \n",
        "        self.features_extractor = FeatureExtractor(\n",
        "            input_size=cnn_input_size, \n",
        "            output_len=cnn_output_len\n",
        "        )\n",
        "        \n",
        "        self.sequence_predictor = SequencePredictor(\n",
        "            input_size=self.features_extractor.num_output_features,\n",
        "            hidden_size=rnn_hidden_size, \n",
        "            num_layers=rnn_num_layers,\n",
        "            num_classes=(len(alphabet) + 1), \n",
        "            dropout=rnn_dropout,\n",
        "            bidirectional=rnn_bidirectional\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        features = self.features_extractor(x)\n",
        "        sequence = self.sequence_predictor(features)\n",
        "        return sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q9skzPR76C53",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pH-uG20N6C57"
      },
      "source": [
        "### c) Обучаем модель для распознавания текста номера"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hg9TzNSR6C58",
        "colab": {}
      },
      "source": [
        "crnn = CRNN()\n",
        "crnn.to(device);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gwpbboAd6C6A",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(crnn.parameters(), lr=1e-3, amsgrad=True, weight_decay=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EZWXOjTyLxhs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "outputId": "d41051fa-debe-4ebb-b1e2-28f200770523"
      },
      "source": [
        "# Здесь тоже оставил пока обучение без валидации. \n",
        "# Может, здесь и стоит валидировать. \n",
        "# Но опять же, 1-2 эпох хватает, а значит модель видит почти все данные \n",
        "# в первый раз и лосс на трейне вполне отражает реальность\n",
        "from torch.nn.functional import ctc_loss, log_softmax\n",
        "\n",
        "best_val_loss = np.inf\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "NUM_EPOCH = 5\n",
        "cur_lr = 1e-3\n",
        "for epoch in range(NUM_EPOCH):\n",
        "  crnn.train()\n",
        "  print(f'\\n\\tEpoch {epoch}:\\tLR={cur_lr*1000} e-3')\n",
        "  epoch_losses = []\n",
        "  print_loss = []\n",
        "  optimizer = torch.optim.Adam(crnn.parameters(), lr=cur_lr, amsgrad=True, weight_decay=1e-5)\n",
        "\n",
        "  for i, batch in enumerate(tqdm.tqdm(train_ocr_loader,\n",
        "                                                  total=len(train_ocr_loader),\n",
        "                                                  leave=False, position=0)):\n",
        "      images = batch[\"image\"].to(device)\n",
        "      seqs_gt = batch[\"seq\"]\n",
        "      seq_lens_gt = batch[\"seq_len\"]\n",
        "\n",
        "      seqs_pred = crnn(images).cpu()\n",
        "      log_probs = F.log_softmax(seqs_pred, dim=2)\n",
        "      seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
        "\n",
        "      loss = F.ctc_loss(\n",
        "          log_probs=log_probs,  # (T, N, C)\n",
        "          targets=seqs_gt,  # N, S or sum(target_lengths)\n",
        "          input_lengths=seq_lens_pred,  # N\n",
        "          target_lengths=seq_lens_gt # N\n",
        "      )  \n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      print_loss.append(loss.item())\n",
        "      mean_loss = np.mean(print_loss)\n",
        "\n",
        "      epoch_losses.append(loss.item())\n",
        "\n",
        "  train_loss.append(mean_loss)\n",
        "  print('Train loss: ', mean_loss)\n",
        "\n",
        "  crnn.eval()\n",
        "  val_losses = []\n",
        "  try:\n",
        "    for i, b in enumerate(tqdm.tqdm(val_ocr_loader,\n",
        "                                                  total=len(val_ocr_loader),\n",
        "                                                  leave=False, position=0)):\n",
        "        images = b[\"image\"].to(device)\n",
        "        seqs_gt = b[\"seq\"]\n",
        "        seq_lens_gt = b[\"seq_len\"]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            seqs_pred = crnn(images).cpu()\n",
        "        log_probs = log_softmax(seqs_pred, dim=2)\n",
        "        seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
        "\n",
        "        loss = ctc_loss(log_probs=log_probs,  # (T, N, C)\n",
        "                        targets=seqs_gt,  # N, S or sum(target_lengths)\n",
        "                        input_lengths=seq_lens_pred,  # N\n",
        "                        target_lengths=seq_lens_gt)  # N\n",
        "        val_losses.append(loss.item())\n",
        "  except:\n",
        "    print('Err')\n",
        "  vl_loss = np.mean(val_losses)\n",
        "  print('\\nValidation loss = ', vl_loss)\n",
        "  val_loss.append(vl_loss)\n",
        "  if vl_loss < best_val_loss:\n",
        "    best_val_loss = vl_loss\n",
        "    with open(f\"ocr_model2.pth\", \"wb\") as fp:\n",
        "      torch.save(crnn.state_dict(), fp)\n",
        "  cur_lr = cur_lr/3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/6007 [00:00<13:30,  7.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\tEpoch 0:\tLR=1.0 e-3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/665 [00:00<01:42,  6.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss:  0.8465900828859384\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/6007 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Err\n",
            "\n",
            "Validation loss =  0.16942615255135593\n",
            "\n",
            "\tEpoch 1:\tLR=0.3333333333333333 e-3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/665 [00:00<01:46,  6.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss:  0.1513488133192504\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/6007 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Err\n",
            "\n",
            "Validation loss =  0.1379396196255595\n",
            "\n",
            "\tEpoch 2:\tLR=0.1111111111111111 e-3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/665 [00:00<01:46,  6.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss:  0.11938458309569791\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/6007 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Err\n",
            "\n",
            "Validation loss =  0.12626656732544392\n",
            "\n",
            "\tEpoch 3:\tLR=0.037037037037037035 e-3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/665 [00:00<01:47,  6.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss:  0.10609394470448351\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/6007 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Err\n",
            "\n",
            "Validation loss =  0.124657404348142\n",
            "\n",
            "\tEpoch 4:\tLR=0.012345679012345678 e-3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/665 [00:00<01:50,  6.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss:  0.09983202023439411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 664/665 [01:39<00:00,  6.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Err\n",
            "\n",
            "Validation loss =  0.12416489253028196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1iQYmf3fcTcS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f5e821f4-4fef-4844-e714-4c647eba23fe"
      },
      "source": [
        "print(f'\\nVal loss = {best_val_loss:.6}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Val loss = 0.124165\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x1-L49_J6C6O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ac82489d-6a78-4bf1-fcbb-b2c74dec6333"
      },
      "source": [
        "crnn.load_state_dict(torch.load('ocr_model2.pth'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A4z8gsrO6C6S",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iGyoArHb6C6W"
      },
      "source": [
        "### d) Наконец, делаем предсказания"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oTDgxEJ46C6X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "f107f34d057049e4a7b446aa98b03f6c",
            "fb9f2991e1f34b79a3fe558f50ee0c81",
            "272f2ff8a89e4628aa3dabb53af9c1f0",
            "d3f29bc1d76e467180789c1a4635d4d5",
            "873b174c370e46fbbc200d32815646fe",
            "a3060d3bf7954da5a107a2d222be6253",
            "f821bb8778e845eabf14b465af6c7985",
            "d9387f668d3e483fa7613c058222b34d"
          ]
        },
        "outputId": "dc086e3f-301b-4575-a317-2fdfe3496751"
      },
      "source": [
        "test_marks = load_json(os.path.join(DATA_PATH, 'test.json'))\n",
        "\n",
        "resizer = Resize()\n",
        "\n",
        "file_name_result = [] \n",
        "plates_string_result = []\n",
        "\n",
        "for item in tqdm.notebook.tqdm(test_marks):\n",
        "\n",
        "    img_path = item[\"file\"]\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    results_to_sort = []\n",
        "    for box in item['nums']:\n",
        "        x_min, y_min, x_max, y_max = box['bbox']\n",
        "        img_bbox = resizer(img[y_min:y_max, x_min:x_max])\n",
        "        img_bbox = my_transforms(img_bbox)\n",
        "        img_bbox = img_bbox.unsqueeze(0)\n",
        "\n",
        "        points = np.clip(np.array(box['box']), 0, None)\n",
        "        img_polygon = resizer(four_point_transform(img, points))\n",
        "        img_polygon = my_transforms(img_polygon)\n",
        "        img_polygon = img_polygon.unsqueeze(0)\n",
        "\n",
        "        preds_bbox = crnn(img_bbox.to(device)).cpu().detach()\n",
        "        preds_poly = crnn(img_polygon.to(device)).cpu().detach()\n",
        "\n",
        "        preds = preds_poly + preds_bbox\n",
        "        num_text = decode(preds, alphabet)[0]\n",
        "\n",
        "        results_to_sort.append((x_min, num_text))\n",
        "\n",
        "    results = sorted(results_to_sort, key=lambda x: x[0])\n",
        "    num_list = [x[1] for x in results]\n",
        "\n",
        "    plates_string = ' '.join(num_list)\n",
        "    file_name = img_path[img_path.find('test/'):]\n",
        "\n",
        "    file_name_result.append(file_name)\n",
        "    plates_string_result.append(plates_string)\n",
        "    \n",
        "df_submit = pd.DataFrame({'file_name': file_name_result, 'plates_string': plates_string_result})\n",
        "df_submit.to_csv('submit4.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f107f34d057049e4a7b446aa98b03f6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=3188.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp5LVgO9eLpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}